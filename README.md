# Gemma-2B-sciq: Investigating the Feasibility, Truthfulness, and Reliability of End-to-End Fine-tuned Large Language Models for Unconstrained Multiple-Choice Quiz Generation

## Gradio demo
A live Gradio demo can be found on [HuggingFace Spaces](https://huggingface.co/spaces/Darwinkel/gemma-2b-sciq). It allows you to input a context and optionally an answer, and generates a question and distractors. The examples are taken from the SciQ test set. 

This should be usable as-is for people who want to quickly create a multiple choice exam for some given contexts.

## Installation
- Run `pip install requirements.txt` to install the required dependencies.
- You need to provide the `HF_TOKEN` environment variable containing your HuggingFace access token to be able to download the Google Gemma base checkpoints. You can do this by running `export HF_TOKEN=your_token` in your terminal, or by prefixing Python commands with `HF_TOKEN=your_token`. Your account must have accepted the Google Gemma terms and conditions.

## Datasets
The [SciQ dataset is pulled from HuggingFace](https://huggingface.co/datasets/allenai/sciq).

## Preprocessing
In principle, no extensive preprocessing is needed for the SciQ dataset. The `format_row*` functions described in `utils.py` perform the basic string reordering necessary for causal language modelling and inference. This is done automatically in the inference and training scripts.

## Inference
Run `python inference.py` or `python inference_qualitative_analysis.py`. Both perform inference on the SciQ test set and save the results to a TSV file. The first is run on a random subset to facilitate qualitative analysis, whereas the latter is run on the entire test set to facilitate quantitative analysis. The TSV files contain the context, answer, question, and distractors generated by the model. They include the ground truth, generations with an answer, and generations without an answer. Pre-generated files are located in the `datasets` folder.

By default, the inference scripts use the fine-tuned adapter model in the `model` folder.

We recommend using the Gradio demo for end-user usage. Alternatively, you can modify either of the inference scripts to your own needs.

## Training
Run `python train.py`. A Habrok jobscript is also provided in this repository. Fine-tuning on a V100 GPU takes around three hours, and checkpoints are written to the `outputs` directory.

## Distractor analysis
The distractor similarity analysis is performed in the `distractor_similarity_analysis.ipynb` Jupyter notebook. This notebook loads the test set and the model output for quantitative analysis. It generates the statistics and the plot used for the report. All relevant output has been retained. 
Run `duplicate_analysis.py` for the quantitative analysis. This script loads a dataset and counts the number of duplicate answers in the distractors and the number of duplicate distractors in the distractors. It returns a short overview of the different amounts.

## Question Evaluation
To run the tests for question evaluation, make sure you are located in the `evaluation` folder. From there, you can run `python question_eval.py` to run the evaluation file. This file contains similarity calculations to evaluate the similarity of the contexts and questions from the original data compared to the generated questions by the trained Gemma model. The output shows the scores of various similarity metrics: Jaccard score, BLEU score, ROUGE score and METEOR score. Additionally, it shows and saves two figures that visualise the calculated jaccard scores. These figures are saved as PNG files in the `evaluation` folder.
